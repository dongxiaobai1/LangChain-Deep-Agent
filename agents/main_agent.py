import json
import os
from agents.subagents.db_operator import DBAgent
from agents.subagents.code_editor import CodeAgent
from agents.subagents.search_agent import SearchAgent
from agents.tools.helper_tools import llm
from langchain_core.messages import SystemMessage

# --- ä» memory åŒ…å¯¼å…¥æ¥å£ ---
from memory import get_history_adapter, format_chat_history

class MainAgent:
    def __init__(self, session_id="default_user"):
        self.name = "æ€»è°ƒåº¦å®˜"
        self.session_id = session_id
        
        # --- 1. è®°å¿†ç³»ç»Ÿåˆå§‹åŒ– ---
        current_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.dirname(current_dir)
        self.memory_dir = os.path.join(root_dir, "memory")
        
        self.history = get_history_adapter(self.memory_dir, session_id)
        
        self.db_agent = DBAgent()
        self.code_agent = CodeAgent()
        self.search_agent = SearchAgent()
        
        self.agents = {
            "æ•°æ®åº“ä¸“å®¶": self.db_agent,
            "ä»£ç è®¡ç®—ä¸“å®¶": self.code_agent,
            "è”ç½‘æœç´¢ä¸“å®¶": self.search_agent
        }

    def run(self, user_query: str):
        context_record = {"original_query": user_query, "intermediate_steps": []}
        
        # --- 2. ä¼˜åŒ–ï¼šå‡å°‘ Planner çœ‹åˆ°çš„å†å²æ·±åº¦ (k=2 è¶³å¤Ÿè¯†åˆ«ä¸Šä¸‹æ–‡ï¼Œåˆä¸ä¼šå¹²æ‰°æ–°è¯é¢˜) ---
        history_str = format_chat_history(self.history.messages, k=2)

        # 3. å¢å¼ºå‹ä»»åŠ¡æ‹†è§£ (Planner) - åŠ å…¥æ„å›¾åˆ†ç±»é€»è¾‘
        planner_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæœ€é«˜çº§åˆ«çš„ä»»åŠ¡è§„åˆ’å®˜ã€‚
        
        [å¯¹è¯å†å²]
        {history_str if history_str else "æ— "}
        
        [å½“å‰é—®é¢˜]
        {user_query}

        --- è§„åˆ’æŒ‡ä»¤ ---
        1. åˆ¤æ–­å½“å‰é—®é¢˜æ˜¯å¦éœ€è¦ã€ä¸“å®¶å›¢é˜Ÿã€‘åä½œï¼ˆå¦‚æŸ¥è¯¢ã€è®¡ç®—ã€æœç´¢ç­‰ï¼‰ã€‚
        2. å¦‚æœåªæ˜¯å¯’æš„ï¼ˆå¦‚â€œä½ å¥½â€ã€â€œè°¢è°¢â€ï¼‰ã€é—²èŠï¼Œæˆ–è€…ç”¨æˆ·æ˜ç¡®æƒ³å¼€å¯ä¸€ä¸ªä¸å†å²æ— å…³çš„æ–°è¯é¢˜ï¼Œè¯·è¿”å›ç©ºåˆ—è¡¨ []ã€‚
        3. å¦‚æœæ˜¯å†å²ä»»åŠ¡çš„æ˜ç¡®å»¶ç»­æˆ–å¤æ‚éœ€æ±‚ï¼Œè¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼æ‹†è§£æ­¥éª¤ã€‚

        å¯ç”¨ä¸“å®¶ï¼š[æ•°æ®åº“ä¸“å®¶], [ä»£ç è®¡ç®—ä¸“å®¶], [è”ç½‘æœç´¢ä¸“å®¶]ã€‚
        è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼å›å¤ï¼Œä¸è¦æœ‰ä»»ä½•å¼€åœºç™½ï¼š
        [
            {{"step": 1, "agent": "...", "task": "..."}}
        ]
        """

        print(f"\nğŸ§  [{self.name}] æ­£åœ¨åˆ†ææ„å›¾ä¸ä»»åŠ¡...")
        res = llm.invoke([SystemMessage(content=planner_prompt)])
        
        # --- æ¸…ç†ä¸è§£æ ---
        raw_content = res.content.strip()
        clean_json = raw_content.replace("```json", "").replace("```", "").strip()
        
        try:
            plan = json.loads(clean_json)
            
            # æ ‡å‡†åŒ– plan æ ¼å¼
            if isinstance(plan, dict):
                plan = plan.get("steps", plan.get("plan", [plan]))
            
            # --- ğŸ’¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¦‚æœæ˜¯é—²èŠæˆ–ç©ºè®¡åˆ’ï¼Œç›´æ¥è¿›å…¥å¯¹è¯æ¨¡å¼ ---
            if not plan or len(plan) == 0:
                print(f"ğŸ’¬ [{self.name}] è¯†åˆ«ä¸ºç®€å•å¯¹è¯ï¼Œç›´æ¥ç”Ÿæˆå›å¤...")
                chat_prompt = f"""
                å‚è€ƒå¯¹è¯å†å²ï¼Œç›´æ¥å›ç­”ç”¨æˆ·ã€‚ä¸è¦æåŠä¸“å®¶æˆ–ä»»åŠ¡æ‹†è§£ã€‚
                [å¯¹è¯å†å²]: {history_str}
                [ç”¨æˆ·]: {user_query}
                """
                final_answer = llm.invoke([SystemMessage(content=chat_prompt)]).content
                
                # å­˜å…¥è®°å¿†å¹¶è¿”å›
                self.history.add_user_message(user_query)
                self.history.add_ai_message(final_answer)
                return final_answer

        except Exception as e:
            print(f"âš ï¸ è§£æå¼‚å¸¸ï¼Œåˆ‡æ¢æœç´¢å…œåº•: {e}")
            return self.search_agent.run(user_query)

        # 4. è¿­ä»£æ‰§è¡Œä¸“å®¶ä»»åŠ¡ (ä»…åœ¨ plan ä¸ä¸ºç©ºæ—¶æ‰§è¡Œ)
        for step in plan:
            if not isinstance(step, dict): continue
                
            agent_key = step.get('agent')
            task_text = step.get('task')
            
            if agent_key in self.agents:
                print(f"ğŸš€ è°ƒåº¦ä¸­: {agent_key} -> ä»»åŠ¡: {task_text}")
                
                steps_history = json.dumps(context_record["intermediate_steps"], ensure_ascii=False)
                task_with_context = f"èƒŒæ™¯å†å²: {steps_history}\nå½“å‰ä»»åŠ¡: {task_text}"
                
                try:
                    result = self.agents[agent_key].run(task_with_context)
                except Exception as e:
                    result = f"æ‰§è¡Œå‡ºé”™: {str(e)}"
                
                context_record["intermediate_steps"].append({
                    "step": step.get('step', 'unknown'), 
                    "agent": agent_key, 
                    "result": result
                })

            # è¯„ä¼°æ˜¯å¦æ»¡è¶³éœ€æ±‚
            assessment_prompt = f"åŸºäºä¿¡æ¯: {json.dumps(context_record['intermediate_steps'], ensure_ascii=False)}, å·²è¶³ä»¥å›ç­” '{user_query}' å—? åªå›å¤ YES æˆ– NO"
            try:
                is_ready = llm.invoke([SystemMessage(content=assessment_prompt)]).content.strip()
                if "YES" in is_ready.upper():
                    break
            except:
                continue

        # 5. æ•´åˆæœ€ç»ˆç­”æ¡ˆ
        print(f"ğŸ¨ [{self.name}] æ­£åœ¨æ±‡æ€»ç»“æœ...")
        synthesis_prompt = f"""
        è¯·æ•´åˆä»¥ä¸‹æ‰§è¡Œè¿‡ç¨‹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š{user_query}ã€‚
        è¿‡ç¨‹ï¼š{json.dumps(context_record['intermediate_steps'], ensure_ascii=False)}
        """
        final_answer = llm.invoke([SystemMessage(content=synthesis_prompt)]).content

        # --- 6. æŒä¹…åŒ–ä¿å­˜ ---
        self.history.add_user_message(user_query)
        self.history.add_ai_message(final_answer)

        return final_answer